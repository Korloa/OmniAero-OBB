# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""åŸºäº CBAM çš„å¤šæ¨¡æ€ç‰¹å¾èåˆæ¨¡å—ã€‚"""

from __future__ import annotations

import torch
import torch.nn as nn

__all__ = ("ChannelAttention", "SpatialAttention", "CBAMFusion")


class ChannelAttention(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—ã€‚

    ä½¿ç”¨å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–å¯¹é€šé“ç‰¹å¾è¿›è¡Œé‡æ–°æ ¡å‡†ã€‚

    å±æ€§:
        avg_pool (nn.AdaptiveAvgPool2d): å…¨å±€å¹³å‡æ± åŒ–å±‚ã€‚
        max_pool (nn.AdaptiveMaxPool2d): å…¨å±€æœ€å¤§æ± åŒ–å±‚ã€‚
        fc1 (nn.Conv2d): ç¬¬ä¸€ä¸ª 1x1 å·ç§¯ï¼Œç”¨äºé€šé“é™ç»´ã€‚
        relu (nn.ReLU): ReLU æ¿€æ´»å‡½æ•°ã€‚
        fc2 (nn.Conv2d): ç¬¬äºŒä¸ª 1x1 å·ç§¯ï¼Œç”¨äºé€šé“å‡ç»´ã€‚
        sigmoid (nn.Sigmoid): Sigmoid æ¿€æ´»å‡½æ•°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æƒé‡ã€‚

    ç¤ºä¾‹:
        >>> ca = ChannelAttention(ch=64)
        >>> x = torch.randn(1, 64, 32, 32)
        >>> out = ca(x)
        >>> print(out.shape)
        torch.Size([1, 64, 32, 32])
    """

    def __init__(self, ch: int, reduction: int = 16):
        """åˆå§‹åŒ–é€šé“æ³¨æ„åŠ›æ¨¡å—ã€‚

        å‚æ•°:
            ch (int): è¾“å…¥é€šé“æ•°ã€‚
            reduction (int): é€šé“é™ç»´æ¯”ä¾‹ï¼Œç”¨äºç“¶é¢ˆç»“æ„ã€‚
        """
        super().__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # å…±äº«çš„ MLPï¼Œé‡‡ç”¨ç“¶é¢ˆç»“æ„
        self.fc1 = nn.Conv2d(ch, ch // reduction, 1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.fc2 = nn.Conv2d(ch // reduction, ch, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å¯¹è¾“å…¥å¼ é‡åº”ç”¨é€šé“æ³¨æ„åŠ›ã€‚

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚

        è¿”å›:
            (torch.Tensor): åº”ç”¨é€šé“æ³¨æ„åŠ›åçš„è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚
        """
        # å¹³å‡æ± åŒ–åˆ†æ”¯
        avg_out = self.fc2(self.relu(self.fc1(self.avg_pool(x))))
        # æœ€å¤§æ± åŒ–åˆ†æ”¯
        max_out = self.fc2(self.relu(self.fc1(self.max_pool(x))))
        # åˆå¹¶å¹¶åº”ç”¨ sigmoid
        out = self.sigmoid(avg_out + max_out)
        return x * out


class SpatialAttention(nn.Module):
    """ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ã€‚

    æ²¿é€šé“ç»´åº¦ä½¿ç”¨å¹³å‡æ± åŒ–å’Œæœ€å¤§æ± åŒ–æ¥åº”ç”¨ç©ºé—´æ³¨æ„åŠ›ã€‚

    å±æ€§:
        conv (nn.Conv2d): 7x7 å·ç§¯ï¼Œç”¨äºç©ºé—´æ³¨æ„åŠ›ã€‚
        sigmoid (nn.Sigmoid): Sigmoid æ¿€æ´»å‡½æ•°ï¼Œç”Ÿæˆæ³¨æ„åŠ›æƒé‡ã€‚

    ç¤ºä¾‹:
        >>> sa = SpatialAttention()
        >>> x = torch.randn(1, 64, 32, 32)
        >>> out = sa(x)
        >>> print(out.shape)
        torch.Size([1, 64, 32, 32])
    """

    def __init__(self, kernel_size: int = 7):
        """åˆå§‹åŒ–ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ã€‚

        å‚æ•°:
            kernel_size (int): å·ç§¯æ ¸å¤§å°ï¼ˆé»˜è®¤: 7ï¼‰ã€‚
        """
        super().__init__()
        assert kernel_size in {3, 7}, "å·ç§¯æ ¸å¤§å°å¿…é¡»æ˜¯ 3 æˆ– 7"
        padding = 3 if kernel_size == 7 else 1
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å¯¹è¾“å…¥å¼ é‡åº”ç”¨ç©ºé—´æ³¨æ„åŠ›ã€‚

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚

        è¿”å›:
            (torch.Tensor): åº”ç”¨ç©ºé—´æ³¨æ„åŠ›åçš„è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚
        """
        # æ²¿é€šé“ç»´åº¦è¿›è¡Œå¹³å‡æ± åŒ–
        avg_out = torch.mean(x, dim=1, keepdim=True)
        # æ²¿é€šé“ç»´åº¦è¿›è¡Œæœ€å¤§æ± åŒ–
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        # æ‹¼æ¥å¹¶åº”ç”¨å·ç§¯
        out = torch.cat([avg_out, max_out], dim=1)
        out = self.sigmoid(self.conv(out))
        return x * out


class CBAMFusion(nn.Module):
    """åŸºäº CBAM çš„å¤šæ¨¡æ€èåˆæ¨¡å—ã€‚

    ä½¿ç”¨å·ç§¯å—æ³¨æ„åŠ›æ¨¡å— (CBAM) èåˆ RGB å’Œ IR ç‰¹å¾ï¼Œä»¥å¢å¼ºå¤šæ¨¡æ€ç‰¹å¾è¡¨ç¤ºã€‚

    è¯¥æ¨¡å—é€šè¿‡ä»¥ä¸‹æ­¥éª¤å¤„ç† 4 é€šé“è¾“å…¥ï¼ˆ3 RGB + 1 IRï¼‰ï¼š
    1. ä½¿ç”¨ 1x1 å·ç§¯åˆ†åˆ«ä» RGB å’Œ IR ä¸­æå–ç‰¹å¾
    2. æ‹¼æ¥ç‰¹å¾
    3. åº”ç”¨é€šé“æ³¨æ„åŠ›æ¥é‡æ–°æ ¡å‡†ç‰¹å¾é€šé“
    4. åº”ç”¨ç©ºé—´æ³¨æ„åŠ›æ¥å¼ºè°ƒé‡è¦çš„ç©ºé—´åŒºåŸŸ
    5. å°†ç»´åº¦é™å›ç›®æ ‡é€šé“æ•°

    å±æ€§:
        conv_rgb (nn.Conv2d): ç”¨äº RGB ç‰¹å¾æå–çš„ 1x1 å·ç§¯ã€‚
        conv_ir (nn.Conv2d): ç”¨äº IR ç‰¹å¾æå–çš„ 1x1 å·ç§¯ã€‚
        bn1 (nn.BatchNorm2d): ç‰¹å¾æå–åçš„æ‰¹å½’ä¸€åŒ–ã€‚
        channel_attention (ChannelAttention): é€šé“æ³¨æ„åŠ›æ¨¡å—ã€‚
        spatial_attention (SpatialAttention): ç©ºé—´æ³¨æ„åŠ›æ¨¡å—ã€‚
        conv_fusion (nn.Conv2d): ç”¨äºé™ç»´çš„ 1x1 å·ç§¯ã€‚
        bn2 (nn.BatchNorm2d): èåˆåçš„æ‰¹å½’ä¸€åŒ–ã€‚
        act (nn.SiLU): SiLU æ¿€æ´»å‡½æ•°ã€‚

    ç¤ºä¾‹:
        >>> fusion = CBAMFusion(ch=64)
        >>> x = torch.randn(2, 4, 640, 640)  # 4é€šé“è¾“å…¥ (RGB+IR)
        >>> out = fusion(x)
        >>> print(out.shape)
        torch.Size([2, 64, 640, 640])
    """

    def __init__(self, ch: int = 64, reduction: int = 16):
        """åˆå§‹åŒ– CBAM èåˆæ¨¡å—ã€‚

        å‚æ•°:
            ch (int): è¾“å‡ºé€šé“æ•°ã€‚
            reduction (int): é€šé“æ³¨æ„åŠ›ç“¶é¢ˆçš„é€šé“é™ç»´æ¯”ä¾‹ã€‚
        """
        super().__init__()
        # ç‰¹å¾æå–å±‚
        self.conv_rgb = nn.Conv2d(3, ch, 1, bias=False)  # RGB: 3 -> ch
        self.conv_ir = nn.Conv2d(1, ch, 1, bias=False)   # IR: 1 -> ch
        self.bn1 = nn.BatchNorm2d(ch * 2)

        # CBAM æ³¨æ„åŠ›æ¨¡å—
        self.channel_attention = ChannelAttention(ch * 2, reduction)
        self.spatial_attention = SpatialAttention(kernel_size=7)

        # èåˆå±‚
        self.conv_fusion = nn.Conv2d(ch * 2, ch, 1, bias=False)  # 2*ch -> ch
        self.bn2 = nn.BatchNorm2d(ch)
        self.act = nn.SiLU()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å¯¹ 4 é€šé“è¾“å…¥åº”ç”¨åŸºäº CBAM çš„èåˆã€‚

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, 4, H, W)ï¼Œå…¶ä¸­å‰ 3 ä¸ªé€šé“æ˜¯ RGBï¼Œ
                ç¬¬ 4 ä¸ªé€šé“æ˜¯ IRã€‚

        è¿”å›:
            (torch.Tensor): èåˆåçš„ç‰¹å¾å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, ch, H, W)ã€‚

        å¼‚å¸¸:
            AssertionError: å¦‚æœè¾“å…¥ä¸æ˜¯ 4 é€šé“ã€‚
        """
        assert x.shape[1] == 4, f"æœŸæœ› 4 é€šé“è¾“å…¥ (RGB+IR)ï¼Œä½†å¾—åˆ° {x.shape[1]} ä¸ªé€šé“"

        # æ­¥éª¤ 1: åˆ†ç¦» RGB å’Œ IR é€šé“
        x_rgb = x[:, :3, :, :]  # [B, 3, H, W]
        x_ir = x[:, 3:4, :, :]  # [B, 1, H, W]

        # æ­¥éª¤ 2: åˆ†åˆ«æå–ç‰¹å¾
        feat_rgb = self.conv_rgb(x_rgb)  # [B, ch, H, W]
        feat_ir = self.conv_ir(x_ir)     # [B, ch, H, W]

        # æ­¥éª¤ 3: æ‹¼æ¥ç‰¹å¾
        feat_concat = torch.cat([feat_rgb, feat_ir], dim=1)  # [B, 2*ch, H, W]
        feat_concat = self.bn1(feat_concat)

        # æ­¥éª¤ 4: åº”ç”¨ CBAM æ³¨æ„åŠ›
        feat_ca = self.channel_attention(feat_concat)  # é€šé“æ³¨æ„åŠ›
        feat_sa = self.spatial_attention(feat_ca)      # ç©ºé—´æ³¨æ„åŠ›

        # æ­¥éª¤ 5: é™ç»´å’Œæ¿€æ´»
        feat_fused = self.conv_fusion(feat_sa)  # [B, ch, H, W]
        feat_fused = self.bn2(feat_fused)
        out = self.act(feat_fused)

        return out


if __name__ == "__main__":
    """æµ‹è¯• CBAM èåˆæ¨¡å—ã€‚"""
    print("æµ‹è¯• CBAM èåˆæ¨¡å—...")

    # æµ‹è¯•é€šé“æ³¨æ„åŠ›
    print("\n1. æµ‹è¯•é€šé“æ³¨æ„åŠ›:")
    ca = ChannelAttention(ch=64)
    x_ca = torch.randn(2, 64, 32, 32)
    out_ca = ca(x_ca)
    print(f"   è¾“å…¥å½¢çŠ¶: {x_ca.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {out_ca.shape}")
    assert out_ca.shape == x_ca.shape, "é€šé“æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
    print("   âœ… é€šé“æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡ï¼")

    # æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›
    print("\n2. æµ‹è¯•ç©ºé—´æ³¨æ„åŠ›:")
    sa = SpatialAttention()
    x_sa = torch.randn(2, 64, 32, 32)
    out_sa = sa(x_sa)
    print(f"   è¾“å…¥å½¢çŠ¶: {x_sa.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {out_sa.shape}")
    assert out_sa.shape == x_sa.shape, "ç©ºé—´æ³¨æ„åŠ›è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
    print("   âœ… ç©ºé—´æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡ï¼")

    # æµ‹è¯• CBAM èåˆ
    print("\n3. æµ‹è¯• CBAM èåˆ:")
    fusion = CBAMFusion(ch=64)
    x_fusion = torch.randn(2, 4, 640, 640)  # 4é€šé“è¾“å…¥ (RGB+IR)
    out_fusion = fusion(x_fusion)
    print(f"   è¾“å…¥å½¢çŠ¶: {x_fusion.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {out_fusion.shape}")
    assert out_fusion.shape == (2, 64, 640, 640), "CBAMèåˆè¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…ï¼"
    print("   âœ… CBAMèåˆæµ‹è¯•é€šè¿‡ï¼")

    # æµ‹è¯•ä¸åŒé€šé“æ•°
    print("\n4. æµ‹è¯•ä¸åŒé€šé“æ•°çš„ CBAM èåˆ:")
    for ch in [32, 64, 128]:
        fusion_test = CBAMFusion(ch=ch)
        x_test = torch.randn(1, 4, 320, 320)
        out_test = fusion_test(x_test)
        print(f"   ch={ch}: {x_test.shape} -> {out_test.shape}")
        assert out_test.shape == (1, ch, 320, 320), f"ch={ch} æ—¶æµ‹è¯•å¤±è´¥"
    print("   âœ… æ‰€æœ‰é€šé“æ•°æµ‹è¯•é€šè¿‡ï¼")

    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•æˆåŠŸé€šè¿‡ï¼")

